
---

# ğŸ“¦ Instacart Retail Data Warehouse & Analytics Engineering Pipeline

**Built from scratch: PostgreSQL, SQL, dbt, and Power BI (DirectQuery on 3M+ rows).**

---

## ğŸš€ Project Purpose

To engineer a retail data warehouse and analytics pipelineâ€”designing every layer (raw, staging, intermediate, marts) from scratch in PostgreSQL, modeling them in dbt Core, and delivering a live Power BI dashboard via DirectQuery.

---

## ğŸ—ï¸ Architecture Overview

![pipeline](images/architecture.png)

---

## ğŸ“Š Dataset

This project uses the public **Instacart Online Grocery Shopping 2017** dataset from Kaggle (3M+ rows of orders, products, and baskets).

---

## ğŸ› ï¸ Tech Stack

* **Python**: Data loading and preprocessing
* **PostgreSQL**: Data warehouse, schemas, indexing, materialized views,
* **SQL**: Transformation logic, KPI calculations.
* **dbt Core**: Staging, intermediate, and mart models
* **Power BI**: DirectQuery dashboards on 3M+ rows
* **Git**: Version control (~70 commits)

---

## ğŸ—„ï¸ 1. Data Warehouse Design

Engineered a complete warehouse architecture in PostgreSQL with raw, staging, intermediate, and mart layers.
Manually designed schemas, implemented primary/foreign keys, and created materialized views for performance.

![Schema](images/schema.png)
![views and mvs](images/views_and_mvs.png)

---

## ğŸ”„ 2. SQL Transformations & Optimization

All business logic was crafted in SQL, including joins, KPIs, and aggregations. Optimized performance with indexing and materialized views before layering dbt on top.

---

## ğŸ§± 3. dbt Modeling

After validating SQL logic, all transformations were modeled in dbt. Developed staging, intermediate, and mart layers with full lineage documentation.dbt/README.md for further references.

![dbt lineage graph](images/dbt_graph.png)

---

## ğŸ“Š 4. Power BI Dashboard (DirectQuery)

Built a 3-page Power BI dashboard connected live via DirectQuery to handle 3M+ rows in real-time. Showcased order KPIs, product insights, and user behavior without import mode. performance screenshots are available in images folder.

![dashboard_pg_1](dashboards/dashboard_1.png)
![dashboard_pg_2](dashboards/dashboard_2.png)
![dashboard_pg_3](dashboards/dashboard_3.png)

---

## ğŸ“‚ Repository Structure

Retail_Analytics_Engineering_Pipeline/
â”œâ”€â”€ dashboards/
â”‚   â”œâ”€â”€ dashboard.pbix
â”‚   â”œâ”€â”€ dashboard.pdf
â”‚   â”œâ”€â”€ dashboard_1.png
â”‚   â”œâ”€â”€ dashboard_2.png
â”‚   â””â”€â”€ dashboard_3.png
â”‚
â”œâ”€â”€ data_raw/
â”‚   â”œâ”€â”€ aisles.csv
â”‚   â”œâ”€â”€ departments.csv
â”‚   â”œâ”€â”€ orders.csv
â”‚   â”œâ”€â”€ order_products__prior.csv
â”‚   â”œâ”€â”€ order_products__train.csv
â”‚   â””â”€â”€ products.csv
â”‚
â”œâ”€â”€ data_clean/
â”‚   â”œâ”€â”€ aisles.csv
â”‚   â”œâ”€â”€ departments.csv
â”‚   â”œâ”€â”€ orders.csv
â”‚   â”œâ”€â”€ order_products.csv
â”‚   â””â”€â”€ products.csv
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_aisles.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_departments.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_orders.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_order_products.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_products.sql
â”‚   â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â”‚   â””â”€â”€ int_order_basket_sizes.sql
â”‚   â”‚   â””â”€â”€ mart/
â”‚   â”‚       â”œâ”€â”€ agg_orders_by_dow.sql
â”‚   â”‚       â”œâ”€â”€ agg_product_metrics.sql
â”‚   â”‚       â”œâ”€â”€ fct_kpi_overview.sql
â”‚   â”‚       â””â”€â”€ fct_orders_by_dow.sql
â”‚   â””â”€â”€ sources.yml
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ architecture.png
â”‚   â”œâ”€â”€ dbt_graph.png
â”‚   â”œâ”€â”€ schema.png
â”‚   â””â”€â”€ views_and_mvs.png
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_explore_raw.ipynb
â”‚   â”œâ”€â”€ 02_clean_transform.ipynb
â”‚   â””â”€â”€ 03_load_to_postgres.ipynb
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ 01_schema.sql
â”‚   â”œâ”€â”€ 02_test_load.sql
â”‚   â”œâ”€â”€ 03_indexes.sql
â”‚   â”œâ”€â”€ 04_analytics_queries.sql
â”‚   â”œâ”€â”€ 05_views.sql
â”‚   â”œâ”€â”€ 06_materialized_views.sql
â”‚   â””â”€â”€ 07_metric_views.sql
â”‚
â””â”€â”€ ETL_RUN_LOG.md

---

## ğŸ“ How to Run Locally

1. Clone the repo and set up the Python environment.
2. Run ETL notebooks to load data into PostgreSQL.
3. Execute dbt models.
4. Open Power BI and connect via DirectQuery.

---

## ğŸ”§ Future Improvements

* Add dbt tests (uniqueness, relationships)
* Deploy dbt to a scheduler like Airflow
* Add incremental models for larger datasets

---

## ğŸ‘¤ Author

Raga, Aspiring Analytics Engineer | Berlin, Germany

---
